{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR4rcaM9cwIY"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RahNCX_eckTd"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install --quiet langchain_community\n",
        "!pip install --quiet pypdf\n",
        "!pip install --quiet langchain_weaviate\n",
        "!pip install --quiet rank_bm25\n",
        "!pip install --quiet bitsandbytes\n",
        "!pip install --quiet accelerate\n",
        "!pip install --quiet transformers datasets accelerate nvidia-ml-py3\n",
        "!pip install --quiet transformers\n",
        "!pip install --quiet numpy==1.24.4\n",
        "!pip install -U sentence-transformers\n",
        "!pip install  --quiet langchain_huggingface\n",
        "!pip install --quiet langchain_experimental langchain_openai\n",
        "# !pip install transformers_stream_generator\n",
        "! pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSj81wkuc1J8"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langsmith\n",
        "\n",
        "# Used for this tutorial; not a requirement for LangGraph\n",
        "%pip install -U langchain_anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D3vogomc3c6"
      },
      "source": [
        "## Library importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LosijyPXc653"
      },
      "outputs": [],
      "source": [
        "# import weaviate\n",
        "# from langchain.retrievers.weaviate_hybrid_search import WeaviateHybridSearchRetriever\n",
        "# from langchain.chains import RetrievalQA\n",
        "# from langchain.llms import OpenAI\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import torch\n",
        "from transformers import ( AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline,TextStreamer,TextIteratorStreamer,AutoModelForSequenceClassification )\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "from typing import Annotated\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from threading import Thread\n",
        "from langchain_core.callbacks import streaming_stdout\n",
        "from langchain_core.prompts import PromptTemplate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnSTXEG7dvsT"
      },
      "source": [
        "## LLM model importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQzPJdz2d8V-"
      },
      "outputs": [],
      "source": [
        "def load_quantized_model(model_name: str):\n",
        "    \"\"\"\n",
        "    model_name: Name or path of the model to be loaded.\n",
        "    return: Loaded quantized model.\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config,)\n",
        "    return model\n",
        "\n",
        "# initializing tokenizer\n",
        "def initialize_tokenizer(model_name: str):\n",
        "    \"\"\"\n",
        "    model_name: Name or path of the model for tokenizer initialization.\n",
        "    return: Initialized tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, return_token_type_ids=False)\n",
        "    tokenizer.bos_token_id = 1  # Set beginning of sentence token id\n",
        "    return tokenizer\n",
        "\n",
        "# loading quantized model\n",
        "model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "model = load_quantized_model(model_path)\n",
        "\n",
        "# initializing tokenizer\n",
        "tokenizer = initialize_tokenizer(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PvVx4p6mTP4"
      },
      "outputs": [],
      "source": [
        "streamer = TextIteratorStreamer(tokenizer)\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    use_cache=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=2048,\n",
        "    do_sample=True,\n",
        "    top_k=5,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    return_full_text=False,\n",
        "    # streamer=streamer\n",
        ")\n",
        "\n",
        "callbacks = [streaming_stdout.StreamingStdOutCallbackHandler()]\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe,pipeline_kwargs = {\"streaming\":True})\n",
        "\n",
        "# llm = HuggingFacePipeline(\n",
        "#     pipeline=pipe,\n",
        "#     # callbacks=callbacks,  # Pass your callbacks here\n",
        "#     streaming=True,  # Enable streaming\n",
        "#     verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti2WXG7qeIog"
      },
      "outputs": [],
      "source": [
        "# template = \"\"\"Question: {question}\n",
        "\n",
        "# Answer: Let's think step by step.\"\"\"\n",
        "# prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# chain = prompt | llm\n",
        "\n",
        "# question = \"What is electroencephalography?\"\n",
        "\n",
        "# for chunk in chain.stream(question):\n",
        "#     if chunk and chunk!=\"\\n\":\n",
        "#         print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osyXn8f0dBaN"
      },
      "source": [
        "## LangGraph defination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1NqCQFDdER7"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "def chatbot(state: State):\n",
        "    final_chunk = \"\"\n",
        "    for data in llm.stream(state[\"messages\"]):\n",
        "        final_chunk += data\n",
        "    print(\"===> \",final_chunk)\n",
        "    print(\"===> \",state[\"messages\"])\n",
        "    # print(state[\"messages\"])\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sZNq0d8fNTm"
      },
      "outputs": [],
      "source": [
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "graph = graph_builder.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkCGw4vbg8-l"
      },
      "outputs": [],
      "source": [
        "results = graph.invoke({\"messages\": (\"user\", \"what is the function calling means ?\")})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ70x53KhBsv"
      },
      "outputs": [],
      "source": [
        "## if llm using alot then what we do is llm caching we can use. rather than simple string\n",
        "## can't we cache embedding it self ? and check how much closer it would be with caching that present making us some kind of threshould for the response ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awoZWwwAiRta"
      },
      "outputs": [],
      "source": [
        "print([data for data in results])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
